{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB_cBgh7F_Hi"
   },
   "source": [
    "# Face recognition with deep learning\n",
    "- Andrew J. Graves\n",
    "- 04/19/21\n",
    "- Run on Google Colab with GPUs\n",
    "\n",
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRshJg4mGGgu",
    "outputId": "0f244d55-a152-4efa-e23c-479c1aa097e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
      "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-xm04vz5r\n",
      "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-xm04vz5r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (2.10.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (7.1.2)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (2.4.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras-vggface==0.6) (3.13)\n",
      "Building wheels for collected packages: keras-vggface\n",
      "  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-vggface: filename=keras_vggface-0.6-cp37-none-any.whl size=8312 sha256=70b573127e3de8be2c64e1f61a3928afce1933c7b2d20be06041e707b30db90d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7xtr2otx/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n",
      "Successfully built keras-vggface\n",
      "Installing collected packages: keras-vggface\n",
      "Successfully installed keras-vggface-0.6\n",
      "Collecting keras_applications\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (2.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.15.0)\n",
      "Installing collected packages: keras-applications\n",
      "Successfully installed keras-applications-1.0.8\n"
     ]
    }
   ],
   "source": [
    "# For VGG-Face transfer learning framework\n",
    "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "!pip install keras_applications\n",
    "\n",
    "# Import modules\n",
    "from google.colab import drive\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UlbRoSzNpv-"
   },
   "source": [
    "The next cell assumes the DeepFake database (deepfake_database.zip) is located within the main directory of your Google Drive, and that you are working on Google Colab. If you need to download the data, the link is [here](https://e.pcloud.link/publink/show?code=XZnsxkZkEAgI1OgQIJHLnNl9ErhV4vpHuV0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ejj09piC0DYN",
    "outputId": "5372f5c3-5b8c-4a63-d7da-407496d74723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "--2021-04-18 18:01:44--  https://github.com/DariusAf/MesoNet/blob/master/weights/MesoInception_DF.h5?raw=true\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/DariusAf/MesoNet/raw/master/weights/MesoInception_DF.h5 [following]\n",
      "--2021-04-18 18:01:44--  https://github.com/DariusAf/MesoNet/raw/master/weights/MesoInception_DF.h5\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/DariusAf/MesoNet/master/weights/MesoInception_DF.h5 [following]\n",
      "--2021-04-18 18:01:44--  https://raw.githubusercontent.com/DariusAf/MesoNet/master/weights/MesoInception_DF.h5\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 197504 (193K) [application/octet-stream]\n",
      "Saving to: ‘MesoInception_DF.h5’\n",
      "\n",
      "MesoInception_DF.h5 100%[===================>] 192.88K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2021-04-18 18:01:44 (44.3 MB/s) - ‘MesoInception_DF.h5’ saved [197504/197504]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assumes deepfake database is located within your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy the zipped data from your Google Drive\n",
    "!cp \"/content/drive/MyDrive/deepfake_database.zip\" .\n",
    "# Unzip the copied data\n",
    "!echo 'N' | unzip -q deepfake_database.zip\n",
    "# Remove the zipped file from Google Collab\n",
    "!rm deepfake_database.zip\n",
    "\n",
    "# Download the DeepFake weights for MesoInception-4\n",
    "!wget -O MesoInception_DF.h5 https://github.com/DariusAf/MesoNet/blob/master/weights/MesoInception_DF.h5?raw=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPFQgrcpJtJm"
   },
   "source": [
    "Here is the code from *classifiers.py* borrowed from this [repository](https://github.com/DariusAf). We are only including their best-performing model, the MesoInception-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5_d_WtpLGyAS"
   },
   "outputs": [],
   "source": [
    "# See https://github.com/DariusAf/MesoNet/blob/master/classifiers.py\n",
    "\n",
    "# We do not need these modules to run our model,\n",
    "# but they are used by MesoInception-4\n",
    "from tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, \\\n",
    "    BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU, Lambda\n",
    "\n",
    "IMGWIDTH = 256\n",
    "\n",
    "class Classifier:\n",
    "    def __init__():\n",
    "        self.model = 0\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self.model.train_on_batch(x, y)\n",
    "    \n",
    "    def get_accuracy(self, x, y):\n",
    "        return self.model.test_on_batch(x, y)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "class MesoInception4(Classifier):\n",
    "    def __init__(self, learning_rate = 0.001):\n",
    "        self.model = self.init_model()\n",
    "        optimizer = Adam(lr = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, loss='mean_squared_error', \n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def InceptionLayer(self, a, b, c, d):\n",
    "        def func(x):\n",
    "            x1 = Conv2D(a, (1, 1), padding='same', activation='relu')(x)\n",
    "            \n",
    "            x2 = Conv2D(b, (1, 1), padding='same', activation='relu')(x)\n",
    "            x2 = Conv2D(b, (3, 3), padding='same', activation='relu')(x2)\n",
    "            \n",
    "            x3 = Conv2D(c, (1, 1), padding='same', activation='relu')(x)\n",
    "            x3 = Conv2D(c, (3, 3), dilation_rate=2, strides=1, \n",
    "                        padding='same', activation='relu')(x3)\n",
    "            \n",
    "            x4 = Conv2D(d, (1, 1), padding='same', activation='relu')(x)\n",
    "            x4 = Conv2D(d, (3, 3), dilation_rate=3, strides=1, \n",
    "                        padding='same', activation='relu')(x4)\n",
    "\n",
    "            y = Concatenate(axis = -1)([x1, x2, x3, x4])\n",
    "            \n",
    "            return y\n",
    "        return func\n",
    "    \n",
    "    def init_model(self):\n",
    "        x = Input(shape = (IMGWIDTH, IMGWIDTH, 3))\n",
    "        \n",
    "        x1 = self.InceptionLayer(1, 4, 4, 2)(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "        \n",
    "        x2 = self.InceptionLayer(2, 4, 4, 2)(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n",
    "        \n",
    "        x3 = Conv2D(16, (5, 5), padding='same', activation='relu')(x2)\n",
    "        x3 = BatchNormalization()(x3)\n",
    "        x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "        x4 = Conv2D(16, (5, 5), padding='same', activation='relu')(x3)\n",
    "        x4 = BatchNormalization()(x4)\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "        \n",
    "        y = Flatten()(x4)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(16)(y)\n",
    "        y = LeakyReLU(alpha=0.1)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "        return KerasModel(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHLfYjQkp-Z-"
   },
   "source": [
    "For convenience we are not going to refit the MesoInception-4, but instead will attempt to replicate their accuracy metrics using their openly available model weights to generate predictions on the validation (test) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04ZkZVimp990",
    "outputId": "48a57558-eae8-4672-d2dc-3ee84015938a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7104 images belonging to 2 classes.\n",
      "\n",
      "Meso-Inception4 Accuracy: 0.913147509098053\n"
     ]
    }
   ],
   "source": [
    "# Specify base directory for database\n",
    "base_dir = '/content/deepfake_database/deepfake_database/'\n",
    "\n",
    "# Build the data generator\n",
    "meso_gen = ImageDataGenerator(rescale=1./255)\n",
    "meso_test = meso_gen.flow_from_directory(\n",
    "        f'{base_dir}validation/',\n",
    "        target_size=(256, 256),\n",
    "        class_mode='binary')\n",
    "\n",
    "# Instantiate the MesoInception4-model\n",
    "meso_inception4 = MesoInception4()\n",
    "# Endow the model with pre-trained weights\n",
    "meso_inception4.load('/content/MesoInception_DF.h5')\n",
    "# Evaluate on the validation (test) set\n",
    "_, meso_acc = meso_inception4.model.evaluate(meso_test, \n",
    "                                             verbose=0)\n",
    "# Print accuracy results (close but not exact replication of paper)\n",
    "print(f'\\nMeso-Inception4 Accuracy: {meso_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lU2dCdkrQMqI"
   },
   "source": [
    "Now we will build our own model class named *TransferLearning*, which will take a different strategy than the original authors. Rather than trying to develop our own convolutional neural network architecture, we will stand on the shoulder of giants and modify pre-trained weights of an existing framework. Specifically, we will use the [Visual Geometry Group](https://www.robots.ox.ac.uk/~vgg/software/vgg_face/) (VGG)-Face weights trained with a ResNet 50 architecture. These weights could prove to be useful for discriminating real from fake faces, given the weights were trained on various face images. We will modify the weights of the upper layers with a slow learning rate to appropriately adapt learning to our current task. We will also use cross-entropy as our loss function rather than mean squared error. We will handle all rescaling/ pre-processing within the model itself, rather than within the image generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1iJj20OALquH"
   },
   "outputs": [],
   "source": [
    "class TransferLearning(Classifier):\n",
    "    def __init__(self, learning_rate=1e-5, eps=1e-9):\n",
    "        self.model = self.init_model()\n",
    "        # Specify low learning rate and low epsilon\n",
    "        optimizer = Adam(lr=learning_rate, epsilon=eps)\n",
    "        # Use binary crossentropy instead of mean squared error\n",
    "        self.model.compile(optimizer=optimizer, \n",
    "                           loss='binary_crossentropy', \n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def init_model(self): \n",
    "        \n",
    "        # Use VGGFace weights for transfer learning\n",
    "        base_model = VGGFace(model='resnet50', weights='vggface', \n",
    "                             pooling='avg', include_top=False)\n",
    "\n",
    "        # Update weights after this layer index\n",
    "        layer_idx = 100\n",
    "        for layer in base_model.layers[:layer_idx]:\n",
    "            # Allow training for all BatchNorm statistics\n",
    "            if layer.__class__.__name__ != 'BatchNormalization':\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Specify input dimensions\n",
    "        x = Input(shape=(img_size, img_size, 3))\n",
    "        # Preprocess for ResNet 50\n",
    "        preproc = preprocess_input(x)\n",
    "        # Feed preprocessed inputs into ResNet 50\n",
    "        res_net = base_model(preproc)\n",
    "\n",
    "        # Apply sigmoid layer on output\n",
    "        y = Dense(1, activation='sigmoid')(res_net)\n",
    "\n",
    "        return KerasModel(inputs=x, outputs=y)\n",
    "\n",
    "# Build a learning rate schedule\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return lr\n",
    "    else:\n",
    "        # Exponentially decay the learning rate\n",
    "        return lr*exp(-0.2)\n",
    "lr_sched = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Apply early stopping\n",
    "patience = 5\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', \n",
    "                           patience=patience, \n",
    "                           restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRIsPosFRlZy"
   },
   "source": [
    "Next, we train our own model to see how its performance compares with MesoInception-4. \n",
    "\n",
    "First, we will find the best number of epochs through early stopping by splitting the training set into two small sets. We expect the validation accuracy in this case to be much smaller, due to the reduced size of the training set. However, it could provide a reasonable estimate of the optimal number of epochs for the entire training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PqyQBeAG0gt",
    "outputId": "8b9d7ecc-10a3-4d33-8e0b-47970b616276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6177 images belonging to 2 classes.\n",
      "Found 6176 images belonging to 2 classes.\n",
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "# Specify size of batches and size of images for ResNet50\n",
    "batch_size = 32\n",
    "img_size = 224\n",
    "\n",
    "# Instantiate the validation data generator (split in half)\n",
    "val_gen = ImageDataGenerator(validation_split=0.5)\n",
    "\n",
    "# Get half of the training set\n",
    "train_half = val_gen.flow_from_directory(\n",
    "        f'{base_dir}train:test/',\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training')\n",
    "\n",
    "# The other half comprises the validation set\n",
    "val = val_gen.flow_from_directory(\n",
    "        f'{base_dir}train:test/',\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation')\n",
    "\n",
    "# Instantiate the transfer learning model\n",
    "tl_val = TransferLearning()\n",
    "# Fit the transfer learning model\n",
    "tl_val.model.fit(train_half, validation_data=val, epochs=30,\n",
    "             callbacks=[lr_sched, early_stop],\n",
    "             verbose=0)\n",
    "# Extract the optimal number of epochs\n",
    "n_epochs = early_stop.stopped_epoch - patience + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb0NLYbpRCYX"
   },
   "source": [
    "Now we will fit the model to the entire training set using the number of epochs found by the previous validation callback. Then we will evaluate the performance of the model on the validation (test) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVKkrBKDdZ2X",
    "outputId": "d7b10d16-ed11-4200-bfae-f3f87a5d1a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12353 images belonging to 2 classes.\n",
      "Found 7104 images belonging to 2 classes.\n",
      "Epoch 1/6\n",
      "387/387 [==============================] - 108s 268ms/step - loss: 0.2400 - accuracy: 0.9081\n",
      "Epoch 2/6\n",
      "387/387 [==============================] - 104s 269ms/step - loss: 0.0086 - accuracy: 0.9980\n",
      "Epoch 3/6\n",
      "387/387 [==============================] - 104s 269ms/step - loss: 0.0029 - accuracy: 0.9998\n",
      "Epoch 4/6\n",
      "387/387 [==============================] - 104s 268ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 5/6\n",
      "387/387 [==============================] - 104s 269ms/step - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 6/6\n",
      "387/387 [==============================] - 104s 268ms/step - loss: 0.0013 - accuracy: 0.9999\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.2238 - accuracy: 0.9369\n",
      "\n",
      "Our Transfer Learning Framework Accuracy: 0.9369369149208069\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the full data generator\n",
    "gen = ImageDataGenerator()\n",
    "\n",
    "# Get the full training set\n",
    "train = gen.flow_from_directory(\n",
    "        f'{base_dir}train:test/',\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Get the full test set\n",
    "test = gen.flow_from_directory(\n",
    "        f'{base_dir}validation/',\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Instantiate the transfer learning model\n",
    "tl = TransferLearning()\n",
    "# Fit the model to the full training set\n",
    "tl.model.fit(train, \n",
    "             # Use number of epochs from early stopping\n",
    "             epochs=n_epochs,\n",
    "             callbacks=[lr_sched])\n",
    "\n",
    "# Final prediction performance (accuracy)\n",
    "_, tl_acc = tl.model.evaluate(test)\n",
    "# Print accuracy results (beats original results)\n",
    "print(f'\\nOur Transfer Learning Framework Accuracy: {tl_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA7SE6-7u-k5"
   },
   "source": [
    "In their paper, the original authors reported $91.7\\%$ for their best accuracy on the individual DeepFake images using their MesoInception-4 model. We were able to *nearly* replicate their findings using their MesoInception-4 pre-trained weights. For fairness, we will compare our results to their published accuracy metrics, rather than what we replicated locally (which was worse than the published score).\n",
    "\n",
    "Our transfer learning framework outperforms their published metric by several percentage points by achieving $\\approx 93.7\\%$ (to be fair, our model has many more parameters than theirs does!). We believe this to be the case simply because transfer learning is a powerful framework when your available data is smaller than data trained by benchmark models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ajg3eh_keh4nb_gjy7kb_codeathon_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
